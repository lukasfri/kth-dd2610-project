{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRivrmgBhBIk"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import flax.linen as nn  # or nnx.Module if you prefer\n",
        "\n",
        "\n",
        "class FiniteScalarQuantizer(nn.Module):\n",
        "    def __init__(self, latent_dim, L, debug=False):\n",
        "        \"\"\"\n",
        "        latent_dim : D (number of channels)\n",
        "        L : iterable of ints, length D, number of levels per dimension\n",
        "        \"\"\"\n",
        "        self.latent_dim = latent_dim\n",
        "        self.L = jnp.array(L, dtype=jnp.int32)\n",
        "        self.debug = debug\n",
        "\n",
        "        # Compute FSQ basis (mixed radix)\n",
        "        # basis[d] = product_{k>d} L[k]\n",
        "        basis_list = []\n",
        "        acc = 1\n",
        "        for l in reversed(L):\n",
        "            basis_list.append(acc)\n",
        "            acc *= l\n",
        "        self._basis = jnp.array(list(reversed(basis_list)), dtype=jnp.int32)\n",
        "\n",
        "        # Store numpy version for easy mod in inverse\n",
        "        self._levels_np = np.array(L, dtype=np.int32)\n",
        "\n",
        "        if self.debug:\n",
        "            print(\"Levels per dim:\", self.L)\n",
        "            print(\"Basis:\", self._basis)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Scale & shift (FSQ uses centered integers)\n",
        "    # ----------------------------------------------------------------------\n",
        "    def _scale_and_shift(self, zhat):\n",
        "        \"\"\"\n",
        "        zhat has integer quantized values in the range [-floor(L/2), floor(L/2)]\n",
        "        Shift them to [0, L-1].\n",
        "        \"\"\"\n",
        "        return zhat + jnp.floor(self.L / 2)\n",
        "\n",
        "    def _scale_and_shift_inverse(self, zhat_shifted):\n",
        "        \"\"\"\n",
        "        Reverse of scale_and_shift:\n",
        "        input in [0, L-1]\n",
        "        output in [-floor(L/2), floor(L/2)]\n",
        "        \"\"\"\n",
        "        return zhat_shifted - jnp.floor(self.L / 2)\n",
        "\n",
        "    # ----------------------------------------------------------------------\n",
        "    # Your quantization function\n",
        "    # ----------------------------------------------------------------------\n",
        "    def round_matrix(self, x):\n",
        "        # Straight-through estimator version (if needed)\n",
        "        # return x + jax.lax.stop_gradient(jnp.round(x) - x)\n",
        "        return jnp.round(x)\n",
        "\n",
        "    def q_func(self, x):\n",
        "        \"\"\"\n",
        "        x: [N, D]\n",
        "        Returns unrounded quantization targets\n",
        "        \"\"\"\n",
        "        L_broadcast = self.L[None, :]\n",
        "        return jnp.tanh(x) * jnp.floor(L_broadcast / 2)\n",
        "\n",
        "    def __call__(self, z):\n",
        "        \"\"\"\n",
        "        z : [B, H, W, D]\n",
        "        returns quantized z_q : [B, H, W, D]\n",
        "        \"\"\"\n",
        "        B, H, W, D = z.shape\n",
        "        assert D == self.latent_dim\n",
        "\n",
        "        z_flat = z.reshape((B * H * W, D))\n",
        "        z_q_flat = self.round_matrix(self.q_func(z_flat))\n",
        "        z_q = z_q_flat.reshape((B, H, W, D))\n",
        "        return z_q\n",
        "\n",
        "    def codes_to_indexes(self, zhat):\n",
        "        \"\"\"\n",
        "        zhat: [..., D] quantized integer codes (e.g., output of your quantizer)\n",
        "        Returns: [...,] integer index for each vector\n",
        "        \"\"\"\n",
        "        assert zhat.shape[-1] == self.latent_dim\n",
        "\n",
        "        # shift from centered integers to [0, L-1]\n",
        "        shifted = self._scale_and_shift(zhat)\n",
        "\n",
        "        # mixed-radix flattening\n",
        "        return (shifted * self._basis).sum(axis=-1).astype(jnp.uint32)\n",
        "\n",
        "    def indexes_to_codes(self, indices):\n",
        "        \"\"\"\n",
        "        indices: [...,] integer indices\n",
        "        Returns: [..., D] quantized integer codes\n",
        "        \"\"\"\n",
        "        # add trailing dimension\n",
        "        idx = indices[..., jnp.newaxis]  # [..., 1]\n",
        "\n",
        "        # mod & floor divide to extract mixed radix digits\n",
        "        codes_non_centered = np.mod(\n",
        "            np.floor_divide(idx, self._basis), \n",
        "            self._levels_np\n",
        "        )\n",
        "\n",
        "        # Shift back to centered integer grid\n",
        "        return self._scale_and_shift_inverse(codes_non_centered)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UYO_sDz-ejSG"
      },
      "outputs": [],
      "source": [
        "# config\n",
        "batch_size = 4\n",
        "z_len = 3   # latent vector length\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG_kP1HldMvs",
        "outputId": "aaba941c-8835-4db5-c786-4b48c437baf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The latent vector matrix should contain 4 vectors of size 3\n",
            "Quantization level [[7 5 9]]\n",
            "Each value of z is mapped from [[-3. -2. -4.]] to [[3. 2. 4.]]\n",
            "Size of codebook : [[343 125 729]]\n",
            "\n",
            "Input z:\n",
            " [[ 0.1  2.  -3. ]\n",
            " [ 1.7 -0.5  0.3]\n",
            " [ 5.   0.  -1. ]\n",
            " [-2.3  4.5  2.2]]\n",
            "\n",
            "Quantized z:\n",
            " [[ 0.  2. -4.]\n",
            " [ 3. -1.  1.]\n",
            " [ 3.  0. -3.]\n",
            " [-3.  2.  4.]]\n",
            "\n",
            "--- Codebook Usage Stats ---\n",
            "Total possible codes (|C|): 315\n",
            "Unique codes used in batch: 4\n",
            "Codebook utilization: 1.27%\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "\n",
        "# -----------------------------------\n",
        "# 1. Dummy latent vectors\n",
        "# -----------------------------------\n",
        "\n",
        "# Random encoded latent vectors\n",
        "z = jnp.array([\n",
        "    [0.1, 2.0, -3.0],\n",
        "    [1.7, -0.5, 0.3],\n",
        "    [5.0, 0.0, -1.0],\n",
        "    [-2.3, 4.5, 2.2]\n",
        "])\n",
        "\n",
        "# -----------------------------------\n",
        "# 2. Per-dimension L (now a column vector)\n",
        "# Example: L = [7, 5, 9]\n",
        "# -----------------------------------\n",
        "L = jnp.array([[7, 5, 9]])   # shape = (1, z_len)\n",
        "\n",
        "# -----------------------------------\n",
        "# 3. Initialize FSQ\n",
        "# -----------------------------------\n",
        "fsq = FSQ(batch_size=batch_size, encoder_length=z_len, L=L, debug=True)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4. Quantize\n",
        "# -----------------------------------\n",
        "z_hat = fsq.quantise(z)\n",
        "\n",
        "print(\"\\nInput z:\\n\", z)\n",
        "print(\"\\nQuantized z:\\n\", z_hat)\n",
        "\n",
        "# -----------------------------------\n",
        "# 5. Codebook usage stats\n",
        "# -----------------------------------\n",
        "\n",
        "# Total codebook size = product of all levels\n",
        "total_codes = int(jnp.prod(L))\n",
        "# Unique codes used in this batch\n",
        "unique_codes = jnp.unique(z_hat, axis=0).shape[0]\n",
        "\n",
        "usage_percentage = (unique_codes / total_codes) * 100\n",
        "\n",
        "print(\"\\n--- Codebook Usage Stats ---\")\n",
        "print(f\"Total possible codes (|C|): {total_codes}\")\n",
        "print(f\"Unique codes used in batch: {unique_codes}\")\n",
        "print(f\"Codebook utilization: {usage_percentage:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BWKMPJVJdS8L"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
